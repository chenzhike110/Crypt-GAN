Today is sunday and tomorrow is monday, I will go to park with my family and it would be a nice holiday. But just one minute ago, we found that today is actually Thursday and there will not be any break tomorrow. What a pity! But I believe that sunday is coming soon.

Recent work has defined an optimal reward problem in which an agent designer, with an objective reward function that evaluates an agentâ€™s behavior, has a choice of what reward function to build into a learning or planning agent to guide its behavior. Existing results on ORP show weak mitigation of limited computational resources, the existence of reward functions so that agents when guided by them do better than when guided by the objective reward function. These existing results ignore the cost of finding such good reward functions. We define a nested optimal reward and control architecture that achieves strong mitigation of limited computational resources. We show empirically that the designer is better off using the new architecture that spends some of its limited resources learning a good reward function instead of using all of its resources to optimize its behavior with respect to the objective reward function.
